{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tian Chen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Import the spam dataset and print the first six rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "from statistics import mean\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make:</th>\n",
       "      <th>word_freq_address:</th>\n",
       "      <th>word_freq_all:</th>\n",
       "      <th>word_freq_3d:</th>\n",
       "      <th>word_freq_our:</th>\n",
       "      <th>word_freq_over:</th>\n",
       "      <th>word_freq_remove:</th>\n",
       "      <th>word_freq_internet:</th>\n",
       "      <th>word_freq_order:</th>\n",
       "      <th>word_freq_mail:</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;:</th>\n",
       "      <th>char_freq_(:</th>\n",
       "      <th>char_freq_[:</th>\n",
       "      <th>char_freq_!:</th>\n",
       "      <th>char_freq_$:</th>\n",
       "      <th>char_freq_#:</th>\n",
       "      <th>capital_run_length_average:</th>\n",
       "      <th>capital_run_length_longest:</th>\n",
       "      <th>capital_run_length_total:</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>15</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make:  word_freq_address:  word_freq_all:  word_freq_3d:  \\\n",
       "0             0.00                0.64            0.64            0.0   \n",
       "1             0.21                0.28            0.50            0.0   \n",
       "2             0.06                0.00            0.71            0.0   \n",
       "3             0.00                0.00            0.00            0.0   \n",
       "4             0.00                0.00            0.00            0.0   \n",
       "5             0.00                0.00            0.00            0.0   \n",
       "\n",
       "   word_freq_our:  word_freq_over:  word_freq_remove:  word_freq_internet:  \\\n",
       "0            0.32             0.00               0.00                 0.00   \n",
       "1            0.14             0.28               0.21                 0.07   \n",
       "2            1.23             0.19               0.19                 0.12   \n",
       "3            0.63             0.00               0.31                 0.63   \n",
       "4            0.63             0.00               0.31                 0.63   \n",
       "5            1.85             0.00               0.00                 1.85   \n",
       "\n",
       "   word_freq_order:  word_freq_mail:  ...  char_freq_;:  char_freq_(:  \\\n",
       "0              0.00             0.00  ...          0.00         0.000   \n",
       "1              0.00             0.94  ...          0.00         0.132   \n",
       "2              0.64             0.25  ...          0.01         0.143   \n",
       "3              0.31             0.63  ...          0.00         0.137   \n",
       "4              0.31             0.63  ...          0.00         0.135   \n",
       "5              0.00             0.00  ...          0.00         0.223   \n",
       "\n",
       "   char_freq_[:  char_freq_!:  char_freq_$:  char_freq_#:  \\\n",
       "0           0.0         0.778         0.000         0.000   \n",
       "1           0.0         0.372         0.180         0.048   \n",
       "2           0.0         0.276         0.184         0.010   \n",
       "3           0.0         0.137         0.000         0.000   \n",
       "4           0.0         0.135         0.000         0.000   \n",
       "5           0.0         0.000         0.000         0.000   \n",
       "\n",
       "   capital_run_length_average:  capital_run_length_longest:  \\\n",
       "0                        3.756                           61   \n",
       "1                        5.114                          101   \n",
       "2                        9.821                          485   \n",
       "3                        3.537                           40   \n",
       "4                        3.537                           40   \n",
       "5                        3.000                           15   \n",
       "\n",
       "   capital_run_length_total:  spam  \n",
       "0                        278     1  \n",
       "1                       1028     1  \n",
       "2                       2259     1  \n",
       "3                        191     1  \n",
       "4                        191     1  \n",
       "5                         54     1  \n",
       "\n",
       "[6 rows x 58 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    'C:/Users/ch199/OneDrive/Columbia University/ML/Midterm/spam_dataset.csv')\n",
    "df.head(6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Read through the documentation of the original dataset here: http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.names. The dependent variable is \"spam\" where one indicates that an email is spam and zero otherwise. Which three variables in the dataset do you think will be important predictors in a model of spam? Why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three variables that I choose are 'word_freq_credit:', 'word_freq_money:', and 'word_freq_order:'. These three variables respectively record the frequency of 'credit', 'money', and 'order'.\n",
    "\n",
    "I think 'credit' is important because often times spam emails are trying to make money from the receivers, so it is highly likely that those emails would include credit-related things to attract the receivers.\n",
    "\n",
    "'money' is important because a large quantity of spam emails are about advertisement of products or paid contents, and they are all about letting the receivers pay money.\n",
    "\n",
    "'order' is important as I have often received spam emails that are like \"order right now and save 80%\", so it is possible that many spam emails are related to the word 'order'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Visualize the univariate distribution of each of the variables in the previous question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x238c56754f0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# the displot of word freq credit\n",
    "sns.displot(x='word_freq_credit:', data=df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x238c5bfc6a0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.displot(df, x='word_freq_money:')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x238c5bfcac0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.displot(df, x='word_freq_order:')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4: Name each of the supervised learning models that we have learned thus far that are used to predict dependent variables like \"spam\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN, Logistic Regression, Logistic Regression with L1/L2 penalty, SVM, decision tree, bagged tree, random forest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5: Describe the importance of training and test data. Why do we separate data into these subsets?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we split data into training group and testing group primarily to make our models work better. If we do not split our data and use the whole data to train our model, then we cannot validate our model by letting it make predictions, since the data that we use to test the model is encompassed in the data that we use to train the model, which would lead the model to make 100% correct predictions. Therefore, in order to see whether our model is capable of making good predictions, we must split our data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6: What is k-fold cross validation and what do we use it for?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-fold cv is a method to validate our models. Instead of splitting our data into training and testing sets, we split the data into K (can be any reasonable number) \"folds\", and we use we use cross validation to evaluate our model. For each fold, we use K-1 groups of data to train the model and use the remaining 1 group to test the model. Then, our model is fitted on training set and tested on test set, and we get the model evaluation scores, which we use to evaluate our model.\n",
    "\n",
    "Cross validation is to remedy the setback of data splitting. Traditinoally, when we split data into training and testing sets, there is only a portion of the data that we can use to train the model. Now, if we use crovss validation, all data can be used for training the model. Thus, we can simultaneously use all of our data and see how well our model performs on unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7: How is k-fold cross validation different from stratified k-fold cross validation?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stratified K-fold cv is different than K-fold cv in that SK-fold \"folds\" data in a way that mimicries the relative class frequencies on the entire dataset (Lecture 3 Slides). That is to say the folds of data retain the proportions of classes in the original dataset.\n",
    "\n",
    "As a result, Stratified folds are representative of the classes in original dataset, and that helps reduce bias and variance in our model as classes in the dataset are less likely to be misrepresented and thus more likely to be fairly computed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following codes are preparations for question 8-11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "df_x = df[['word_freq_credit:', 'word_freq_money:', 'word_freq_order:']]\n",
    "y = df['spam']\n",
    "X = df_x\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale and KFold\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "scaler.fit(X_test)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "kf = KFold(n_splits=5, random_state=35, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8: Choose one model from question four. Split the data into training and test subsets. Build a model with the three variables in the dataset that you think will be good predictors of \"spam\". Describe why you chose any particular parameters for your model (e.g.- if you used KNN how did you decide to choose a specific value for k). Run the model and evaluate prediction error in two ways: A) On test data directly and B) using k-fold cross-validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My model is KNNClassifier.\n",
    "\n",
    "I choose n_neighbors = 9 because my GridSearchCV, which I have set to avoid even numbers of n_neighbors, shows that 9 is the best parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('standardscaler', StandardScaler()), ('kneighborsclassifier', KNeighborsClassifier())]\n",
      "{'kneighborsclassifier__n_neighbors': 9}\n",
      "0.7567332754126846\n"
     ]
    }
   ],
   "source": [
    "# KNN GridSearchCV\n",
    "knn_pipe = make_pipeline(StandardScaler(), KNeighborsClassifier())\n",
    "print(knn_pipe.steps)\n",
    "\n",
    "param_grid = {'kneighborsclassifier__n_neighbors': range(1, 15, 2)}\n",
    "grid = GridSearchCV(knn_pipe, param_grid, cv=10)\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "print(grid.best_params_)\n",
    "print(grid.score(X_test_scaled, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.766\n",
      "Test set accuracy: 0.755\n",
      "EachCV:\n",
      "[0.75942029 0.74347826 0.78550725 0.76376812 0.74057971]\n",
      "KFold:\n",
      "0.759\n"
     ]
    }
   ],
   "source": [
    "# KNN evaluation\n",
    "# w/o cv\n",
    "knn = KNeighborsClassifier(n_neighbors=9)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Training set accuracy: {:.3f}\".format(\n",
    "    knn.score(X_train_scaled, y_train)))\n",
    "print(\"Test set accuracy: {:.3f}\".format(knn.score(X_test_scaled, y_test)))\n",
    "\n",
    "#w/ cv\n",
    "print(\"EachCV:\\n{}\".format(cross_val_score(\n",
    "    KNeighborsClassifier(n_neighbors=9), X_train_scaled, y_train, cv=kf)))\n",
    "print(\"KFold:\\n{:.3f}\".format(\n",
    "    mean(cross_val_score(KNeighborsClassifier(n_neighbors=9), X_train_scaled, y_train, cv=kf))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9: Choose a second model from question four. Using the same three variables in the dataset that you think will be good predictors of \"spam\". Describe why you chose any particular parameters for your model (e.g.- if you used KNN how did you decide to choose a specific value for k). Run the model and evaluate prediction error in two ways: A) On test data directly and B) using k-fold cross-validation. Did this model predict test data better than your previous model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My model is L1 Logsitic Regression.\n",
    "\n",
    "My C is 1.31. I choose this C because it is the best parameter generated by GridSearchCV.\n",
    "\n",
    "L1 Logistic Regression did not show improvements over KNNClassifier. On the contrary, it performed worse than KNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best mean cross-validation score: 0.747\n",
      "best parameters: {'C': 1.31, 'penalty': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "# L1 G\n",
    "param_l1 = {'C': np.arange(.01, 10, .1), 'penalty': ['l1']}\n",
    "\n",
    "grid_l1 = GridSearchCV(LogisticRegression(solver='liblinear'), param_l1, cv=kf)\n",
    "grid_l1.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"best mean cross-validation score: {:.3f}\".format(grid_l1.best_score_))\n",
    "print(\"best parameters: {}\".format(grid_l1.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso.coef_:\n",
      "[[1.91050093 1.04836275 0.31322716]]\n",
      "Training set score: 0.748\n",
      "Test set score: 0.745\n",
      "EachCV:\n",
      "[0.76231884 0.72753623 0.75942029 0.75362319 0.73188406]\n",
      "KFold:\n",
      "0.747\n"
     ]
    }
   ],
   "source": [
    "# L1 evaluation\n",
    "# w/o cv\n",
    "l1 = LogisticRegression(C=1.31, penalty='l1', solver='liblinear',\n",
    "                        max_iter=10000).fit(X_train_scaled, y_train)\n",
    "print(\"Lasso.coef_:\\n{}\".format(l1.coef_))\n",
    "print(\"Training set score: {:.3f}\".format(l1.score(X_train_scaled, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(l1.score(X_test_scaled, y_test)))\n",
    "\n",
    "#w/ cv\n",
    "l1_cv = cross_val_score(LogisticRegression(\n",
    "    C=1.31, penalty='l1', solver='liblinear', max_iter=10000), X_train_scaled, y_train, cv=kf)\n",
    "print(\"EachCV:\\n{}\".format(l1_cv))\n",
    "print(\"KFold:\\n{:.3f}\".format(mean(l1_cv)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10: Choose a third model from question four. Using the same three variables in the dataset that you think will be good predictors of \"spam\". Describe why you chose any particular parameters for your model (e.g.- if you used KNN how did you decide to choose a specific value for k). Run the model and evaluate prediction error in two ways: A) On test data directly and B) using k-fold cross-validation. Did this model predict test data better than your previous models?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My model is L2 Logistic Regression.\n",
    "\n",
    "My C is 2.81. I choose this C because it is the best parameter generated by GridSearchCV.\n",
    "\n",
    "L2 performed slightly better than L1, but it is not as good as KNNClassifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best mean cross-validation score: 0.748\n",
      "best parameters: {'C': 2.81, 'penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "# L2 G\n",
    "param_l2 = {'C': np.arange(.01, 10, .1), 'penalty': ['l2']}\n",
    "\n",
    "grid_l2 = GridSearchCV(LogisticRegression(solver='newton-cg'), param_l2, cv=kf)\n",
    "grid_l2.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"best mean cross-validation score: {:.3f}\".format(grid_l2.best_score_))\n",
    "print(\"best parameters: {}\".format(grid_l2.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge.coef_:\n",
      "[[1.91062332 1.05006155 0.31435695]]\n",
      "Training set score: 0.748\n",
      "Test set score: 0.745\n",
      "EachCV:\n",
      "[0.76376812 0.72898551 0.75942029 0.75362319 0.73188406]\n",
      "KFold:\n",
      "0.748\n"
     ]
    }
   ],
   "source": [
    "# L2 evaluation\n",
    "# w/o cv\n",
    "l2 = LogisticRegression(C=2.81, penalty='l2', solver='newton-cg',\n",
    "                        max_iter=10000).fit(X_train_scaled, y_train)\n",
    "print(\"Ridge.coef_:\\n{}\".format(l2.coef_))\n",
    "print(\"Training set score: {:.3f}\".format(l2.score(X_train_scaled, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(l2.score(X_test_scaled, y_test)))\n",
    "\n",
    "#w/ cv\n",
    "l2_cv = cross_val_score(LogisticRegression(\n",
    "    C=2.81, penalty='l2', solver='newton-cg', max_iter=10000), X_train_scaled, y_train, cv=kf)\n",
    "print(\"EachCV:\\n{}\".format(l2_cv))\n",
    "print(\"KFold:\\n{:.3f}\".format(mean(l2_cv)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11: Choose a fourth model from question four. Using the same three variables in the dataset that you think will be good predictors of \"spam\". Describe why you chose any particular parameters for your model (e.g.- if you used KNN how did you decide to choose a specific value for k). Run the model and evaluate prediction error in two ways: A) On test data directly and B) using k-fold cross-validation. Did this model predict test data better than your previous models?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My model is Logistic Regression without penalty.\n",
    "\n",
    "There is no tuning parameter for Logistic Regression without penalty.\n",
    "\n",
    "Logistic Regression is outperformed by every other models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logreg .coef_:\n",
      "[[1.81500449 1.04615769 0.31655346]]\n",
      "Training set score: 0.745\n",
      "Test set score: 0.745\n",
      "EachCV:\n",
      "[0.76376812 0.72753623 0.75652174 0.74927536 0.73188406]\n",
      "KFold:\n",
      "0.746\n"
     ]
    }
   ],
   "source": [
    "# Log Reg Evaluation\n",
    "# w/o cv\n",
    "logreg = LogisticRegression(max_iter=10000).fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"logreg .coef_:\\n{}\".format(logreg.coef_))\n",
    "print(\"Training set score: {:.3f}\".format(\n",
    "    logreg.score(X_train_scaled, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg.score(X_test_scaled, y_test)))\n",
    "\n",
    "#w/ cv\n",
    "logcv = cross_val_score(LogisticRegression(\n",
    "    max_iter=10000), X_train_scaled, y_train, cv=kf)\n",
    "print(\"EachCV:\\n{}\".format(logcv))\n",
    "print(\"KFold:\\n{:.3f}\".format(mean(logcv)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12: Now rerun your best model from questions 8 through 11, but this time add three new variables to the model that you think will increase prediction accuracy. Did this model predict test data better than your previous models?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model is KNNClassifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the three additional variables, the score of the new KNN model improved. The model predicted test data better than my previous models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split and scaling\n",
    "df_x2 = df[['word_freq_credit:', 'word_freq_money:', 'word_freq_order:',\n",
    "            'word_freq_your:', 'word_freq_our:', 'capital_run_length_average:']]\n",
    "y = df['spam']\n",
    "X2 = df_x2\n",
    "X2_train, X2_test, y_train, y_test = train_test_split(X2, y, random_state=35)\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X2_train)\n",
    "scaler.fit(X2_test)\n",
    "X2_train_scaled = scaler.transform(X2_train)\n",
    "X2_test_scaled = scaler.transform(X2_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.857\n",
      "Test set accuracy: 0.829\n",
      "EachCV:\n",
      "[0.84927536 0.82173913 0.84202899 0.85217391 0.81304348]\n",
      "KFold:\n",
      "0.836\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "# w/o cv\n",
    "knn_6 = KNeighborsClassifier(n_neighbors=9)\n",
    "knn_6.fit(X2_train_scaled, y_train)\n",
    "\n",
    "print(\"Training set accuracy: {:.3f}\".format(\n",
    "    knn_6.score(X2_train_scaled, y_train)))\n",
    "print(\"Test set accuracy: {:.3f}\".format(knn_6.score(X2_test_scaled, y_test)))\n",
    "\n",
    "#w/ cv\n",
    "print(\"EachCV:\\n{}\".format(cross_val_score(KNeighborsClassifier(\n",
    "    n_neighbors=9), X2_train_scaled, y_train, cv=kf)))\n",
    "print(\"KFold:\\n{:.3f}\".format(\n",
    "    mean(cross_val_score(KNeighborsClassifier(n_neighbors=9), X2_train_scaled, y_train, cv=kf))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13: Rerun all your other models with this final set of six variables, evaluate prediction error, and choose a final model. Why did you select this model among all of the models that you ran?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would select KNNClassifier as my final model.\n",
    "\n",
    "The reason is that with six variables, it has the best scores in training set accuracy, test set accuracy, and cross validation model evaluation score among all models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge.coef_:\n",
      "[[1.46822895 0.65643011 0.20980407 0.75502584 0.5516681  2.69428779]]\n",
      "Training set score: 0.810\n",
      "Test set score: 0.801\n",
      "EachCV:\n",
      "[0.82173913 0.77536232 0.81449275 0.8115942  0.79130435]\n",
      "KFold:\n",
      "0.803\n"
     ]
    }
   ],
   "source": [
    "# L2\n",
    "# w/o cv\n",
    "l2_6 = LogisticRegression(C=2.81, penalty='l2', solver='newton-cg',\n",
    "                          max_iter=10000).fit(X2_train_scaled, y_train)\n",
    "print(\"Ridge.coef_:\\n{}\".format(l2_6.coef_))\n",
    "print(\"Training set score: {:.3f}\".format(\n",
    "    l2_6.score(X2_train_scaled, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(l2_6.score(X2_test_scaled, y_test)))\n",
    "\n",
    "#w/ cv\n",
    "l2_cv_6 = cross_val_score(LogisticRegression(\n",
    "    C=2.81, penalty='l2', solver='newton-cg', max_iter=10000), X2_train_scaled, y_train, cv=kf)\n",
    "print(\"EachCV:\\n{}\".format(l2_cv_6))\n",
    "print(\"KFold:\\n{:.3f}\".format(mean(l2_cv_6)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso.coef_:\n",
      "[[1.45586902 0.65370171 0.20814478 0.75403555 0.55014738 2.71059191]]\n",
      "Training set score: 0.810\n",
      "Test set score: 0.800\n",
      "EachCV:\n",
      "[0.82028986 0.77536232 0.81594203 0.8115942  0.79275362]\n",
      "KFold:\n",
      "0.803\n"
     ]
    }
   ],
   "source": [
    "# L1\n",
    "# w/o cv\n",
    "l1_6 = LogisticRegression(C=1.31, penalty='l1', solver='liblinear',\n",
    "                          max_iter=10000).fit(X2_train_scaled, y_train)\n",
    "print(\"Lasso.coef_:\\n{}\".format(l1_6.coef_))\n",
    "print(\"Training set score: {:.3f}\".format(\n",
    "    l1_6.score(X2_train_scaled, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(l1_6.score(X2_test_scaled, y_test)))\n",
    "\n",
    "#w/ cv\n",
    "l1_cv_6 = cross_val_score(LogisticRegression(\n",
    "    C=1.31, penalty='l1', solver='liblinear', max_iter=10000), X2_train_scaled, y_train, cv=kf)\n",
    "print(\"EachCV:\\n{}\".format(l1_cv_6))\n",
    "print(\"KFold:\\n{:.3f}\".format(mean(l1_cv_6)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logreg .coef_:\n",
      "[[1.40424262 0.65779346 0.2133687  0.75456193 0.55107367 2.60162343]]\n",
      "Training set score: 0.808\n",
      "Test set score: 0.801\n",
      "EachCV:\n",
      "[0.8173913  0.77826087 0.81304348 0.8115942  0.78550725]\n",
      "KFold:\n",
      "0.801\n"
     ]
    }
   ],
   "source": [
    "# Log Reg\n",
    "# w/o cv\n",
    "logreg_6 = LogisticRegression(max_iter=10000).fit(X2_train_scaled, y_train)\n",
    "\n",
    "print(\"logreg .coef_:\\n{}\".format(logreg_6.coef_))\n",
    "print(\"Training set score: {:.3f}\".format(\n",
    "    logreg_6.score(X2_train_scaled, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg_6.score(X2_test_scaled, y_test)))\n",
    "\n",
    "#w/ cv\n",
    "logcv_6 = cross_val_score(LogisticRegression(\n",
    "    max_iter=10000), X2_train_scaled, y_train, cv=kf)\n",
    "print(\"EachCV:\\n{}\".format(logcv_6))\n",
    "print(\"KFold:\\n{:.3f}\".format(mean(logcv_6)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14: What variable that currently is not in your model, if included, would be likely to increase your final model's predictive power? For this answer try to speculate about a variable outside the variables available in the data that would improve you model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One variable that can be useful is the email clients. With this variable being present, one can build a model in a model-based perspective, such as looking at the differences between the frequencies of a certain vocabulary that appear in spam emails that are sent to Gmail users and Outlook users. Thus, our model can learn clients-specific spams and, consequently, more accurately predict spam.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15: Lastly, you have listed each of the models that we have learned to use to predict dependent variables like spam. List each model we have focused on in class thus far that you could use to evaluate data with a continuous dependent variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN, linear regression, Ridge, Lasso, SVM, decision tree, bagged tree, random forest\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b96b0ec23799406967d3f4dd0683001034882b59242e4bd0b6916b0a0b0b8c31"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
